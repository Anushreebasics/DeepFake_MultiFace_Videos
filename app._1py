from flask import Flask, request, jsonify, redirect
import os
import uuid
import cv2
import numpy as np
from tensorflow import keras  # Assuming you're using TensorFlow 2.x with Keras
from keras.applications import inception_v3

IMG_SIZE = 224
BATCH_SIZE = 64
EPOCHS = 10

MAX_SEQ_LENGTH = 20
NUM_FEATURES = 2048

# Flask app setup
app = Flask(__name__)

# Video upload configuration (adjust as needed)
MODEL_PATH = 'DeepFake_MultiFace_Videos/my_model.h5'
UPLOAD_FOLDER = 'DeepFake_MultiFace_Videos/uploads'
ALLOWED_EXTENSIONS = {'mp4', 'avi'}

def allowed_file(filename):
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

# Function to crop center square from a frame
def crop_center_square(frame):
    y, x = frame.shape[0:2]
    min_dim = min(y, x)
    start_x = (x // 2) - (min_dim // 2)
    start_y = (y // 2) - (min_dim // 2)
    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]

# Video processing and prediction function
def predict_on_video(video_path, model_path):
    # Load the pre-trained feature extractor (assuming it's part of your model)
    model = keras.models.load_model(model_path)

    # Load the video
    frames = load_video(video_path)

    # Preprocess the video (assuming preprocessing is done within the model)
    processed_frames = frames  # Modify this line if preprocessing is needed

    # Extract features from the video frames
    if isinstance(model.layers[0], inception_v3.InceptionV3):
        # Assuming InceptionV3 is used for feature extraction
        preprocess_input = inception_v3.preprocess_input
        processed_frames = preprocess_input(processed_frames)
    features = model.predict(processed_frames[None, ...])  # Add batch dimension

    # Assuming your model has a single output neuron for classification:
    prediction = model.predict_classes(features)[0]  # Get the class index
    prediction_proba = model.predict_proba(features)[0]  # Get class probabilities

    # Map class index to label (modify these labels based on your model's output)
    label = "Real" if prediction == 0 else "Deep Fake"

    return label, prediction_proba

# Video loading function
def load_video(path, resize=(IMG_SIZE, IMG_SIZE)):
    cap = cv2.VideoCapture(path)
    frames = []
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frame = crop_center_square(frame)
            frame = cv2.resize(frame, resize)
            frame = frame[:, :, [2, 1, 0]]  # BGR to RGB
            frames.append(frame)
    finally:
        cap.release()
    return np.array(frames)

def prepare_video(video_path, root_dir, MAX_SEQ_LENGTH, NUM_FEATURES):
    """
    Prepares a single video for the sequence model.

    Args:
        video_path (str): Path to the video file.
        root_dir (str): Root directory containing the video.
        MAX_SEQ_LENGTH (int): Maximum sequence length for the model.
        NUM_FEATURES (int): Number of features extracted from each frame.

    Returns:
        tuple: A tuple containing (frame_features, frame_mask)
            - frame_features (np.array): Array of extracted features (shape: (MAX_SEQ_LENGTH, NUM_FEATURES))
            - frame_mask (np.array): Array of booleans indicating masked timesteps (shape: (MAX_SEQ_LENGTH,))
    """

    # Full video path
    full_path = os.path.join(root_dir, video_path)

    # Load frames
    frames = load_video(full_path)
    frames = frames[None, ...]  # Add batch dimension

    # Initialize feature and mask arrays
    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32")
    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")

    # Extract features from frames and create mask
    video_length = frames.shape[0]
    length = min(MAX_SEQ_LENGTH, video_length)
    for j in range(length):
        # Preprocess individual frame (important for InceptionV3)
        preprocessed_frame = preprocess_input(frames[None, j, :])
        frame_features[0, j, :] = feature_extractor.predict(preprocessed_frame)
        frame_mask[0, j] = 1  # 1 = not masked

    return frame_features.squeeze(), frame_mask.squeeze()

# Flask routes

@app.route('/summary', methods=['POST'])
def predict_deepfake():
    if request.method == 'POST':
        video_file = request.files['video']
        if video_file and allowed_file(video_file.filename):
            video_path = os.path.join(UPLOAD_FOLDER, video_file.filename)
            video_file.save(video_path)

            # Prepare video for prediction
            features, mask = prepare_video(video_path, UPLOAD_FOLDER, MAX_SEQ_LENGTH, NUM_FEATURES)

            # Load the pre-trained model
            model = keras.models.load_model(MODEL_PATH)

            # Predict using the model
            predictions = model.predict((features[None, ...], mask[None, ...]))

            # Assuming binary classification (0: Real, 1: Deepfake)
            is_deepfake = int(predictions[0][0] > 0.5)

            return jsonify({'is_deepfake': is_deepfake})
        else:
            return jsonify({'error': 'Invalid file format'}), 400

    return jsonify({'error': 'No video uploaded'}), 400

if __name__ == '__main__':
    app.run(host='127.0.0.1', port=5000)













































# import os
# from flask import Flask, request, jsonify
# from flask_cors import CORS
# from keras.models import load_model
# from keras.preprocessing import image
# from tensorflow import keras
# import cv2
# import numpy as np

# app = Flask(__name__)
# CORS(app)

# MODEL_PATH = 'DeepFake_MultiFace_Videos\\my_model.h5'
# UPLOAD_FOLDER = 'DeepFake_MultiFace_Videos\\uploads'
# ALLOWED_EXTENSIONS = {'mp4', 'avi', 'mpeg', 'mov'}

# model = load_model(MODEL_PATH)
# model.make_predict_function()

# def allowed_file(filename):
#     return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

# IMG_SIZE = 224
# BATCH_SIZE = 64
# EPOCHS = 10

# MAX_SEQ_LENGTH = 20
# NUM_FEATURES=2048

# # def preprocess_and_predict(video_bytes):
# #     # Load the video using OpenCV
# #     video_data = cv2.VideoCapture(cv2.imdecode(np.frombuffer(video_bytes, np.uint8), cv2.IMREAD_COLOR))

# #     # If model expects preprocessed frames, handle that here (if not done within the model)
# #     # ...

# #     # Pass preprocessed data (or raw video if model handles it) to the model
# #     predictions = model.predict(video_data)

# #     return predictions

# def crop_center_square(frame):
#     y, x = frame.shape[0:2]
#     min_dim = min(y, x)
#     start_x = (x // 2) - (min_dim // 2)
#     start_y = (y // 2) - (min_dim // 2)
#     return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]


# def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):
#     cap = cv2.VideoCapture(path)
#     frames = []
#     try:
#         while True:
#             ret, frame = cap.read()
#             if not ret:
#                 break
#             frame = crop_center_square(frame)
#             frame = cv2.resize(frame, resize)
#             frame = frame[:, :, [2, 1, 0]]
#             frames.append(frame)

#             if len(frames) == max_frames:
#                 break
#     finally:
#         cap.release()
#     return np.array(frames)

# # def build_feature_extractor():
# #     feature_extractor = keras.applications.InceptionV3(
# #         weights="imagenet",
# #         include_top=False,
# #         pooling="avg",
# #         input_shape=(IMG_SIZE, IMG_SIZE, 3),
# #     )
# #     preprocess_input = keras.applications.inception_v3.preprocess_input

# #     inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))
# #     preprocessed = preprocess_input(inputs)

# #     outputs = feature_extractor(preprocessed)
# #     return keras.Model(inputs, outputs, name="feature_extractor")


# # feature_extractor = build_feature_extractor()

# # def prepare_video(video_path, root_dir, MAX_SEQ_LENGTH, NUM_FEATURES, feature_extractor):
# #   """
# #   Prepares a single video for the sequence model.

# #   Args:
# #       video_path (str): Path to the video file.
# #       root_dir (str): Root directory containing the video.
# #       MAX_SEQ_LENGTH (int): Maximum sequence length for the model.
# #       NUM_FEATURES (int): Number of features extracted from each frame.
# #       feature_extractor (func): Function that takes a video frame and returns features.

# #   Returns:
# #       tuple: A tuple containing (frame_features, frame_mask, label)
# #           - frame_features (np.array): Array of extracted features (shape: (MAX_SEQ_LENGTH, NUM_FEATURES))
# #           - frame_mask (np.array): Array of booleans indicating masked timesteps (shape: (MAX_SEQ_LENGTH,))
# #           - label (int): Label of the video (0 or 1, depending on your implementation)
# #   """

# #   # Full video path
# #   full_path = os.path.join(root_dir, video_path)

# #   # Load frames
# #   frames = load_video(full_path)
# #   frames = frames[None, ...]  # Add batch dimension

# #   # Initialize feature and mask arrays
# #   frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32")
# #   frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")

# #   # Extract features and create mask
# #   video_length = frames.shape[0]
# #   length = min(MAX_SEQ_LENGTH, video_length)
# #   for j in range(length):
# #     frame_features[0, j, :] = feature_extractor.predict(frames[None, j, :])
# #     frame_mask[0, j] = 1  # 1 = not masked

# #   # Assuming you have a function to get the label based on video path (modify as needed)
# #   label = None  # Replace with your label retrieval function

# #   return frame_features.squeeze(), frame_mask.squeeze(), label

# # @app.route('/summary', methods=['POST'])
# # def process_video():
# #     try:
# #         if 'video' not in request.files:
# #             return jsonify({'error': 'No video provided'}), 400

# #         video_file = request.files['video']
# #         # video_bytes = request.files['video'].read()
        

# #         # if video_file.filename == '':
# #         #     return jsonify({'error': 'No selected file'}), 400

# #         if video_file and allowed_file(video_file.filename):
# #             video_path = os.path.join(UPLOAD_FOLDER, 'uploaded_video.mp4')
# #             video_file.save(video_path)
# #             PREDS=prepare_video('uploaded_video.mp4',UPLOAD_FOLDER,MAX_SEQ_LENGTH, NUM_FEATURES,feature_extractor)


# #             # cap = cv2.VideoCapture(video_path)
# #             # frames = []
# #             # while True:
# #             #     ret, frame = cap.read()
# #             #     if not ret:
# #             #         break
# #             #     preprocessed_frame = preprocess_frame(frame)
# #             #     frames.append(preprocessed_frame)
# #             # cap.release()

# #             # Pass each preprocessed frame to your model for prediction
# #             # predictions = []
# #             # for frame in frames:
# #             #     prediction = model.predict(frame)  # Adjust this line according to how your model accepts in  put
# #             #     predictions.append(prediction)

# #             # Aggregate the predictions if needed
# #             # For example, calculate the average prediction confidence
# #         avg_prediction=model.predict(PREDS)

# #         return jsonify({'message': avg_prediction}), 200
# #         # else:
# #         #     return jsonify({'error': 'Invalid file format'}), 400
# #     except Exception as e:
# #         return jsonify({'error': str(e)}), 500


# from tensorflow import keras  # Assuming you're using TensorFlow 2.x with Keras

# def build_feature_extractor():
#   """
#   Creates a feature extractor model using InceptionV3.
#   """
#   feature_extractor = keras.applications.InceptionV3(
#       weights="imagenet",
#       include_top=False,
#       pooling="avg",
#       input_shape=(IMG_SIZE, IMG_SIZE, 3),
#   )
#   preprocess_input = keras.applications.inception_v3.preprocess_input

#   inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))
#   preprocessed = preprocess_input(inputs)

#   outputs = feature_extractor(preprocessed)
#   return keras.Model(inputs, outputs, name="feature_extractor")

# # Create the feature extractor instance
# from tensorflow import keras
# feature_extractor = build_feature_extractor()
# from keras.applications.inception_v3 import preprocess_input 
# from keras import backend as K

# def preprocess_input(x):
#   """
#   Preprocesses input data for InceptionV3 model.

#   Args:
#       x (np.array): Input data (images) with shape (..., height, width, channels).

#   Returns:
#       np.array: Preprocessed input data with the same shape as the input.
#   """

#   # Ensure channels are last (assuming BGR or other format)
#   if K.image_data_format() == 'channels_first':
#     x = x.transpose(0, 3, 1, 2)

#   # Convert to float32 for calculations
#   x = x.astype('float32')

#   # Subtract mean values (imagenet specific)
#   x -= np.array([[[123.68, 116.779, 103.939]]])

#   # Scale by factor (imagenet specific)
#   x *= np.array([[[1./255., 1./255., 1./255.]]])

#   return x
# def prepare_video(video_path, root_dir, MAX_SEQ_LENGTH, NUM_FEATURES):
#   """
#   Prepares a single video for the sequence model.

#   Args:
#       video_path (str): Path to the video file.
#       root_dir (str): Root directory containing the video.
#       MAX_SEQ_LENGTH (int): Maximum sequence length for the model.
#       NUM_FEATURES (int): Number of features extracted from each frame.

#   Returns:
#       tuple: A tuple containing (frame_features, frame_mask)
#           - frame_features (np.array): Array of extracted features (shape: (MAX_SEQ_LENGTH, NUM_FEATURES))
#           - frame_mask (np.array): Array of booleans indicating masked timesteps (shape: (MAX_SEQ_LENGTH,))
#   """

#   # Full video path
#   full_path = os.path.join(root_dir, video_path)

#   # Load frames
#   frames = load_video(full_path)
#   frames = frames[None, ...]  # Add batch dimension

#   # Initialize feature and mask arrays
#   frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype="float32")
#   frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype="bool")

#   # Extract features from frames and create mask
#   video_length = frames.shape[0]
#   length = min(MAX_SEQ_LENGTH, video_length)
#   for j in range(length):
#     # Preprocess individual frame (important for InceptionV3)
#     preprocessed_frame = preprocess_input(frames[None, j, :])
#     frame_features[0, j, :] = feature_extractor.predict(preprocessed_frame)
#     frame_mask[0, j] = 1  # 1 = not masked

#   return frame_features.squeeze(), frame_mask.squeeze()

# # In your Flask route
# @app.route('/summary', methods=['POST'])
# def predict():
#     if request.method == 'POST':
#         video_file = request.files['video']
#         video_path = video_file.filename
#         video_file.save(os.path.join(UPLOAD_FOLDER, video_path))

#         # Prepare video
#         features, mask = prepare_video(video_path, UPLOAD_FOLDER, MAX_SEQ_LENGTH, NUM_FEATURES)

#         # Load your model (.h5 file)
#         model = keras.models.load_model('model.h5')  # Replace with your model loading logic

#         # Use features and mask for prediction with your model
#         predictions = model.predict((features[None, ...], mask[None, ...]))  # Add batch dimensions

#         # Process prediction results (assuming probability output)
#         is_deepfake = int(predictions[0][0] > 0.5)  # Threshold-based prediction

#         # Return prediction result
#         return jsonify({'is_deepfake': is_deepfake})
#     return 'No video uploaded'




# if __name__ == '__main__':
#     app.run(host='127.0.0.1', port=5000)
